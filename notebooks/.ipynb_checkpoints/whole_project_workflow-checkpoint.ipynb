{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Statistical constraints on observing low-angle normal fault seismicity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "\n",
      "The recognition of low-angle normal faults, or LANFs, with fault dips <30$^\\circ$ in the geologic record and their hypothesized role in accommodating large-magnitude continental extension [1] has been one of the most important developments in tectonics over the past several decades.\n",
      "However, despite widespread field observations of inactive LANFs[2] and their central role in modern extensional tectonic theory[3], they remain enigmatic and contentious structures. \n",
      "This is for two reasons: because brittle faulting on LANFs is in apparent conflict with standard rock mechanical theory as typically applied to the upper crust [4,5], and because observations of active faulting on LANFs is sparse and at times ambiguous[6,7].\n",
      "A considerable amount of research has been performed to address the former concern, reconciling LANF slip with rock mechanics.\n",
      "The latter issue, the paucity of observations, has inhibited hypothesis testing of LANF fault theory, and has also contributed to a mode of thought where the absence of evidence is taken as evidence of absence [8].\n",
      "Alternately, the lack of observed seismic slip on a continental LANF may be explained by the rarity of seismicity and the small number of potential active structures.\n",
      "In this work, we choose to directly address the question of whether the lack of observed seismicity may be interpreted as an indication that LANFs may not slip seismically, or an effect of a small sample size of LANFs that show typical seismic behavior. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Over the past several decades, many field studies found evidence for LANF activity in orogens throughout the world.  These studies typically find arrays of Quaternary normal fault scarps on the fault traces and/or in the hanging walls of mapped or inferred low-angle detachment faults [e.g, refs].  Some studies also have bedrock thermochronology data from the exhumed footwalls of the detachments that is suggestive of ongoing rapid exhumation, although this data does not preclude a recent cessation of faulting.  In some cases, additional evidence for LANF activity comes from geophysical data such as GPS geodesy [Hreinsdottir et al. 20xx] and seismic waves [Doser 1987].  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we compile a dataset from continental LANFs inferred to be active, and directly estimate the likelihood of observing an earthquake on them, using constraints on their geometry and slip rates. We therefore assume each LANF is active, and exhibits typical seismic behavior for a continental normal fault.  Because we require accurate information on fault geometry and slip rate, we limit the study to continental, subareal LANFs, and do not consider those at mid-oceanic ridges or in submerged, attenuated continental crust, even though these have been described and may be numerous."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make map?  Or just insert one..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Monte Carlo simulations of earthquake sequences on individual faults\n",
      "\n",
      "In order to robustly determine the probability that an earthquake on any individual LANF will be observed, we create synthetic sequences of earthquakes.  Each sequence is composed of 20,000 events  M >= 5.0.  Each earthquake is separated from the previous earthquake by the amount of time necessary to build up sufficient strain for an earthquake of that size. The maximum size of earthquakes in the sequences are set to correspond to 15 m of slip on the fault.\n",
      "\n",
      "500 sequences are generated for each fault, with variable fault dip and slip rate (called Ddot from $\\dot{D}$ after fault slip $D$).  In each iteration, dip and Ddot are sampled from uniform distributions based on geologic and geodetic infomation.  This is to ensure that uncertainty in these critical parameters is incorporated into the final probability distributions.\n",
      "\n",
      "Then, the probability of observing an earthquake above a magnitude $M_{min}$ in an observation window of time $t$ (which is in contiguous years) is calculated by running a rolling maximum function of length $t$ over the earthquake time series (including interseismic years), and counting what fraction of time windows contain a large enough earthquake out of the total number of time windows."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All work here is done in Python.  The code snippets below are a complete and functional version of the Python script used to do the calculations; however, a supplementary Python module called 'eq_stats' was made."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setting up the problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Import necessary modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "%config InlineBackend.figure_format = 'retina'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('../eq_stats')\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import eq_stats as eqs\n",
      "import time\n",
      "from joblib import Parallel, delayed\n",
      "from itertools import chain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Read in fault data table\n",
      "\n",
      "Makes a Pandas dataframe of fault data (length, slip rates, etc.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = pd.read_csv('../data/lanf_stats.csv', index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Define some variables to be used later"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_eq_samp = 2e4 # number of earthquakes in time series\n",
      "time_window = np.hstack( (1, np.arange(5, 105, step=5) ) ) # observation times\n",
      "mc_iters = 5e2 # number of Monte Carlo iterations\n",
      "mc_index = np.arange(mc_iters, dtype='int')\n",
      "mc_cols = ['dip', 'Ddot'] + [t for t in time_window]\n",
      "max_eq_slip = 15 #m\n",
      "Mc = 7.64 # Hypothetical corner magnitude for Continental Rift Boundaries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make list of minimum search magnitude $M_{min}$, and then make MultiIndex for Pandas dataframes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_M_list = [5, 5.5, 6, 6.5, 7, 7.5]\n",
      "\n",
      "df_ind_tuples = [[i, M] for i in mc_index for M in min_M_list]\n",
      "df_multi_ind = pd.MultiIndex.from_tuples(df_ind_tuples, names=['mc_iter','M'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Define a function for Joblib Parallel to calculate probabilities for each iteration.\n",
      "\n",
      "Function is defined here so it can access all variables generated by script, not just passed variables.  This makes the code cleaner even if it's not very abstracted.\n",
      "\n",
      "Here is what this function does:\n",
      "\n",
      "- Get the dip, Ddot and maximum earthquake magnitude for each iteration.\n",
      "- Take this info and make the earthquake sequence:\n",
      "    - Take the max earthquake magnitude and make a frequency-magnitude distribution based on a Gutenburg-Richter exponential model.\n",
      "    - Take 50k samples from this distribution, \n",
      "- Make an earthquake time series form the EQ sequence\n",
      "    - Calculate the interseismic strain accumulation time for each event\n",
      "    - Separate each earthquake in the sequence with the appropriate number of years with no events.\n",
      "- Calculate the probability of observation\n",
      "    - Run a rolling maximum for each $t$ in [1, 5, 10, 15, ..., 95, 100]\n",
      "    - Calculate the observation probability above $M_{min}$ in [5, 5.5, 6, 6.5, 7, 7.5]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_iter_probs(iter):\n",
      "    df_iter = fdf.loc[iter].copy()\n",
      "    df_iter['dip'] = mc_d['dip_samp'][iter]\n",
      "    df_iter['Ddot'] = mc_d['Ddot_samp'][iter]\n",
      "\n",
      "    # Generate EQ sample/sequence from F(M) dist.\n",
      "    m_vec = np.linspace(5, mc_d['max_M'][iter], num=1000)\n",
      "    fm_vec = eqs.F(m_vec, Mc=Mc)\n",
      "    M_samp = eqs.sample_from_pdf(m_vec, fm_vec, n_eq_samp)\n",
      "    Mo_samp = eqs.calc_Mo_from_M(M_samp)\n",
      "    \n",
      "    # Make time series of earthquakes, including no eq years\n",
      "    recur_int = eqs.calc_recurrence_interval(Mo=Mo_samp, \n",
      "                                             dip=mc_d['dip_samp'][iter],\n",
      "                                             slip_rate=mc_d['Ddot_samp'][iter],\n",
      "                                             L=params['L_km'],\n",
      "                                             z=params['z_km'])\n",
      "\n",
      "    cum_yrs = eqs.calc_cumulative_yrs(recur_int)\n",
      "    eq_series = eqs.make_eq_time_series(M_samp, cum_yrs)\n",
      "    \n",
      "    # calculate probability of observing EQ in time_window\n",
      "    for t in time_window:\n",
      "        roll_max = pd.rolling_max(eq_series, t)\n",
      "        df_iter[t] = (eqs.get_probability_above_value(roll_max, min_M_list)\n",
      "                      * mc_d['dip_frac'])\n",
      "\n",
      "    return df_iter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Iterate through the faults in the fault database, doing all the calculations for each.\n",
      "\n",
      "The setup of this for loop is basically this:\n",
      "\n",
      "- Make DataFrame for each fault.\n",
      "    - Columns are dip, Ddot, and observation time windows.\n",
      "    - Rows are values for each Monte Carlo iteration.  Values for time windows are calculated probabilities.\n",
      "    \n",
      "- Calculate maximum earthquake magnitude for each MC iteration.\n",
      "\n",
      "- Run the above 'calc_iter_probs' function (parallelized over the MC iterations) and concatenate the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fault in list(f.index):\n",
      "    fdf = pd.DataFrame(index=df_multi_ind, columns=mc_cols, dtype='float')\n",
      "    params = f.loc[fault]\n",
      "    mc_d = {}\n",
      "    mc_d['dip_samp'], mc_d['dip_frac'] = eqs.dip_rand_samp( params['dip_deg'], \n",
      "                                                         params['dip_err_deg'], \n",
      "                                                         mc_iters)\n",
      "\n",
      "    mc_d['Ddot_samp'] = eqs.Ddot_rand_samp(params['slip_rate_mm_a'],\n",
      "                                           params['sr_err_mm_a'], mc_iters)\n",
      "\n",
      "    mc_d['max_Mo'] = eqs.calc_Mo_from_fault_params(L=params['L_km'], \n",
      "                                                   z=params['z_km'], \n",
      "                                                   dip=mc_d['dip_samp'], \n",
      "                                                   D=max_eq_slip)\n",
      "\n",
      "    mc_d['max_M'] = eqs.calc_M_from_Mo(mc_d['max_Mo'])\n",
      "\n",
      "    t0 = time.time()\n",
      "    prob_list = Parallel(n_jobs=-3)( delayed( calc_iter_probs)(ii) \n",
      "                                    for ii in mc_index)\n",
      "    print 'done with', fault, 'parallel calcs in {} s'.format((time.time()-t0))\n",
      "    for ii in mc_index:\n",
      "        fdf.loc[ii][:] = prob_list[ii]\n",
      "    fdf.to_csv('../results/{}_test.csv'.format(fault))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with kongur_shan parallel calcs in 51.947163105 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with leo_pargil parallel calcs in 418.76543808 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with gurla_mandhata parallel calcs in 113.975022078 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with s_lunggar parallel calcs in 241.274301052 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with n_lunggar parallel calcs in 259.45785284 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done with pqx_qingdu parallel calcs in 836.985168934 s\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Examining individual fault results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Load datasets into Pandas dataframes\n",
      "\n",
      "test with one:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ks = pd.read_csv('../results/kongur_shan_test.csv', index_col=[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_xarray = np.tile(time_window, (mc_iters,1))\n",
      "\n",
      "tw_xarray.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_cols = list(time_window.astype('str'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ks_5 = ks[ks.M==5]\n",
      "ks_55 = ks[ks.M==5.5]\n",
      "ks_6 = ks[ks.M==6]\n",
      "ks_65 = ks[ks.M==6.5]\n",
      "ks_7 = ks[ks.M==7]\n",
      "ks_75 = ks[ks.M==7.5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(tw_xarray.T, ks_5[tw_cols].T, 'b', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, ks_55[tw_cols].T, 'g', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, ks_6[tw_cols].T, 'k', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, ks_65[tw_cols].T, 'r', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, ks_7[tw_cols].T, 'c', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, ks_75[tw_cols].T, 'orange', lw=0.5, alpha=0.1)\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Combining results to find joint probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to calculate the probability that we see an earthquake on *any* fault, we make an assumption that will allow us to specify the particular joint probability rule we will need.  We will assume that:\n",
      "\n",
      "The probability $p$ of observing an earthquake on one fault is *independent* of observing an earthquake on another fault.\n",
      "\n",
      "Given that most of the faults here are far from each other, and we have no knowledge of how more adjacent faults will interact, this assumption is the best that we can make."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Joint probabilities for independent events\n",
      "If earthquakes on faults are independent and uncorrelated:\n",
      "\n",
      "$p_{AT}$ := probability of event occuring in time window on one fault (AT)\n",
      "\n",
      "$q_{AT} = 1 - p_{AT} $ := probability of event not occuring in time window on one fault (AT)\n",
      "\n",
      "$p_{AT \\,or\\,LP} = 1 - (q_{AT} \\cdot q_{LP})$\n",
      "\n",
      "There are other ways to calculate it than using $q$, but this method allows us to easily expand to more independent events:\n",
      "\n",
      "$p_{AT \\, or \\, LP\\, or NLR\\, or DV} = 1 - (q_{AT} \\cdot q_{LP} \\cdot q_{NLR} \\cdot q_{DV})$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Calculating these probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "now load the rest:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gm = pd.read_csv('../results/gurla_mandhata_test.csv', index_col=0)\n",
      "slr = pd.read_csv('../results/s_lunggar_test.csv', index_col=0)\n",
      "nlr = pd.read_csv('../results/n_lunggar_test.csv', index_col=0)\n",
      "pqxn = pd.read_csv('../results/pqx_n_test.csv', index_col=0)\n",
      "pqxq = pd.read_csv('../results/pqx_qingdu_test.csv', index_col=0)\n",
      "lp = pd.read_csv('../results/leo_pargil_test.csv', index_col=0)\n",
      "nqtl = pd.read_csv('../results/nqtl_test.csv', index_col=0)\n",
      "at = pd.read_csv('../results/alto_tiberina_test.csv', index_col=0)\n",
      "dv = pd.read_csv('../results/death_valley_test.csv', index_col=0)\n",
      "pv = pd.read_csv('../results/panamint_valley_test.csv', index_col=0)\n",
      "ls = pd.read_csv('../results/laguna_salada_test.csv', index_col=0)\n",
      "dxv = pd.read_csv('../results/dixie_valley_test.csv', index_col=0)\n",
      "dd = pd.read_csv('../results/dayman_dome_test.csv', index_col=0)\n",
      "pp = pd.read_csv('../results/papangeo_test.csv', index_col=0)\n",
      "tk = pd.read_csv('../results/tokorondo_test.csv', index_col=0)\n",
      "cb = pd.read_csv('../results/cordillera_blanca_test.csv', index_col=0)\n",
      "kz = pd.read_csv('../results/kuzey_test.csv', index_col=0)\n",
      "gn = pd.read_csv('../results/guney_test.csv', index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "make list of faults"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_list = ['ks', 'lp', 'gm', 'slr', 'nlr', 'pqxn', 'pqxq', 'nqtl', 'at',\n",
      "          'dv', 'pv', 'ls', 'dxv', 'dd', 'pp', 'tk', 'cb', 'kz', 'gn']\n",
      "\n",
      "fault_list = [ks, lp, gm, slr, nlr, pqxn, pqxq, nqtl, at,\n",
      "              dv, pv, ls, dxv, dd, pp, tk, cb, kz, gn]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Calculate $q$, probability of *not* observing an earthquake, for each fault"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "q = {}\n",
      "\n",
      "for i, f_ in enumerate(f_list):\n",
      "    q[f_] = fault_list[i].copy()\n",
      "    q[f_][tw_cols] = 1 - q[f_][tw_cols]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Now estimate joint probabilities\n",
      "\n",
      "Make list of columns to retain in final dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prob_cols = list( chain( *['M', list(tw_cols)] ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "calculate $p_{joint}$ as $1-(q \\cdot q \\cdot q...)$ and make final dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_prob = 1- np.product([qq[tw_cols] for qq in  q.values()])\n",
      "\n",
      "all_prob['M'] = ks['M']\n",
      "\n",
      "all_prob = all_prob.reindex_axis(prob_cols, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Make probability dataframes for each $M_{MIN}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_p_5 = all_prob[all_prob.M==5]\n",
      "all_p_55 = all_prob[all_prob.M==5.5]\n",
      "all_p_6 = all_prob[all_prob.M==6]\n",
      "all_p_65 = all_prob[all_prob.M==6.5]\n",
      "all_p_7 = all_prob[all_prob.M==7]\n",
      "all_p_75 = all_prob[all_prob.M==7.5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(tw_xarray.T, all_p_5[tw_cols].T, 'b', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, all_p_55[tw_cols].T, 'c', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, all_p_6[tw_cols].T, 'g', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, all_p_65[tw_cols].T, 'gold', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, all_p_7[tw_cols].T, 'orange', lw=0.5, alpha=0.1)\n",
      "\n",
      "plot(tw_xarray.T, all_p_75[tw_cols].T, 'r', lw=0.5, alpha=0.1)\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}